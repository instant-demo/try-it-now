{"id":"demo-multi-plexer-0ga","title":"Fail instance acquisition when Caddy route creation fails","description":"## Problem\nIn `internal/pool/impl.go:98-107`, when Caddy route creation fails, the code logs a warning but still returns the instance to the client. The client receives a URL that doesn't work.\n\n## Impact\n- Client gets broken instance URL\n- Instance stuck in assigned state\n- Poor user experience\n- No automatic recovery\n\n## File to Modify\n- `internal/pool/impl.go` - Lines 98-127\n\n## Current Code (BROKEN)\n```go\n// Line 106-109 - Warns but continues\nif err := m.proxy.AddRoute(ctx, route); err != nil {\n    m.logger.Warn(\"Failed to add route for instance\", \"id\", instance.ID, \"error\", err)\n}\nreturn instance, nil  // Returns broken instance!\n```\n\n## Fixed Code\n```go\n// Add route - FAIL if this fails\nif err := m.proxy.AddRoute(ctx, route); err != nil {\n    m.logger.Error(\"Failed to add route for instance, releasing\", \"id\", instance.ID, \"error\", err)\n    // Cleanup: release the instance back to pool or mark as failed\n    if releaseErr := m.Release(ctx, instance.ID); releaseErr != nil {\n        m.logger.Error(\"Failed to release instance after route failure\", \"id\", instance.ID, \"error\", releaseErr)\n    }\n    return nil, fmt.Errorf(\"failed to create route for instance: %w\", err)\n}\nreturn instance, nil\n```\n\n## Implementation Steps\n1. Change the `m.logger.Warn` to `m.logger.Error` for route failure\n2. Add cleanup: call `m.Release(ctx, instance.ID)` to return instance to pool\n3. Return an error instead of nil: `return nil, fmt.Errorf(\"failed to create route: %w\", err)`\n4. Add a new domain error: `ErrRouteCreationFailed` in `internal/domain/errors.go`\n5. Handle this error in the HTTP handler with appropriate status code (503)\n\n## Verification\n1. Run `make test` - all tests pass\n2. Start infrastructure: `make infra-up`\n3. Stop Caddy: `docker stop demo-multiplexer-caddy-1`\n4. Try to acquire: `curl -X POST http://localhost:8080/api/v1/demo/acquire`\n5. Verify response is 503, not 200 with broken URL\n6. Restart Caddy and verify normal operation resumes\n\n## Dependencies\nNone","status":"open","priority":1,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:10.674614+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:06.359671+04:00","comments":[{"id":7,"issue_id":"demo-multi-plexer-0ga","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m 'fix: fail acquisition when Caddy route creation fails - Closes: demo-multi-plexer-0ga'\n\n2. **Report**: bd comments add demo-multi-plexer-0ga '## Done: \u003csummary of changes and files modified\u003e'\n\n3. **Close**: bd close demo-multi-plexer-0ga --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add kickoff context to next issue: bd comments add \u003cnext-id\u003e '## Kickoff: Previous=0ga, Start at=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:47Z"}]}
{"id":"demo-multi-plexer-1py","title":"Create shared HTTP client for container health checks","description":"## Problem\nNew `http.Client` created on every health check call in both Docker and Podman runtimes. During container provisioning, this happens every 1 second for up to 2 minutes per container.\n\n## Impact\n- Connection pooling defeated\n- GC pressure from allocating/freeing clients\n- Potential file descriptor exhaustion under load\n\n## Files to Modify\n- `internal/container/docker.go` - Lines 238-240, add field to struct\n- `internal/container/podman.go` - Line 335, add field to struct\n\n## Current Code (docker.go:238-240)\n```go\nfunc (r *DockerRuntime) waitForReady(ctx context.Context, port int) error {\n    httpClient := \u0026http.Client{  // ← Created every call\n        Timeout: 5 * time.Second,\n    }\n    // ... used in loop\n}\n```\n\n## Fixed Code\n\n1. Add field to DockerRuntime struct:\n```go\ntype DockerRuntime struct {\n    // ... existing fields\n    healthClient *http.Client  // Shared HTTP client for health checks\n}\n```\n\n2. Initialize in NewDockerRuntime:\n```go\nfunc NewDockerRuntime(...) (*DockerRuntime, error) {\n    r := \u0026DockerRuntime{\n        // ... existing fields\n        healthClient: \u0026http.Client{\n            Timeout: 5 * time.Second,\n            Transport: \u0026http.Transport{\n                MaxIdleConns:        10,\n                IdleConnTimeout:     30 * time.Second,\n                DisableKeepAlives:   false,\n            },\n        },\n    }\n    return r, nil\n}\n```\n\n3. Use in waitForReady:\n```go\nfunc (r *DockerRuntime) waitForReady(ctx context.Context, port int) error {\n    // Use shared client instead of creating new one\n    resp, err := r.healthClient.Get(url)\n    // ...\n}\n```\n\n4. Apply same pattern to PodmanRuntime\n\n## Implementation Steps\n1. Add `healthClient *http.Client` field to DockerRuntime struct\n2. Initialize healthClient in NewDockerRuntime with proper transport settings\n3. Replace local client creation in waitForReady with r.healthClient\n4. Repeat steps 1-3 for PodmanRuntime in podman.go\n5. Consider adding Close() method to cleanup client if needed\n\n## Verification\n1. Run `make test` - all tests pass\n2. Start server: `make dev`\n3. Provision multiple containers and monitor file descriptor count\n4. Verify health checks still work correctly\n5. Check logs for no HTTP client errors\n\n## Dependencies\nNone","status":"open","priority":2,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:07.307882+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:06.574031+04:00","comments":[{"id":14,"issue_id":"demo-multi-plexer-1py","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-1py'\n\n2. **Report**: bd comments demo-multi-plexer-1py --add '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-1py --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments \u003cnext-id\u003e --add '## Kickoff: Previous=demo-multi-plexer-1py, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:52Z"}]}
{"id":"demo-multi-plexer-2kg","title":"Configure Gin trusted proxies to prevent IP spoofing","description":"## Problem\nRate limiting relies on `c.ClientIP()` which trusts X-Forwarded-For header by default. Attackers can spoof their IP to bypass rate limits.\n\n## File to Modify\n- `internal/api/handler.go` - Router setup around line 40\n\n## Current Code\n```go\n// Line 40\nr := gin.New()\n// No trusted proxies configured - trusts all X-Forwarded-For headers\n```\n\n## Fixed Code\n```go\nr := gin.New()\n// Only trust proxy headers from known load balancers\n// In production, set to actual proxy IPs\nr.SetTrustedProxies([]string{\"127.0.0.1\", \"::1\"})\n```\n\n## Implementation Steps\n1. Add `TrustedProxies []string` to config struct in `internal/config/config.go`\n2. Load from `TRUSTED_PROXIES` env var (comma-separated)\n3. Default to `[\"127.0.0.1\", \"::1\"]` for development\n4. Call `r.SetTrustedProxies(cfg.TrustedProxies)` in handler.go after creating router\n5. Update .env.example with `TRUSTED_PROXIES=127.0.0.1,::1`\n\n## Verification\n1. Run `make test` - all tests pass\n2. Start server and verify X-Forwarded-For from untrusted source is ignored\n3. Test: `curl -H \"X-Forwarded-For: 1.2.3.4\" http://localhost:8080/api/v1/demo/acquire`\n4. Verify rate limit uses actual client IP, not spoofed header\n\n## Dependencies\nNone","status":"open","priority":2,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:06.217011+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:04.772195+04:00","comments":[{"id":13,"issue_id":"demo-multi-plexer-2kg","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-2kg'\n\n2. **Report**: bd comments demo-multi-plexer-2kg --add '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-2kg --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments \u003cnext-id\u003e --add '## Kickoff: Previous=demo-multi-plexer-2kg, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:51Z"}]}
{"id":"demo-multi-plexer-339","title":"Add explanatory comments to all silent error suppressions","description":"## Problem\nMultiple locations use `_ = ` to suppress errors without explaining why. This makes it unclear if errors are intentionally ignored or accidentally dropped.\n\n## Impact\n- Code reviewers can't verify intentionality\n- Future maintainers may not understand the decision\n- Hidden bugs can lurk in suppressed errors\n\n## Files to Modify\n- `internal/pool/impl.go` - Lines 353, 367-368, 381-382\n- `internal/api/handler.go` - Line 231\n- `internal/container/docker.go` - Line 126\n- `cmd/server/main.go` - godotenv.Load() error\n\n## Locations to Document\n\n### 1. pool/impl.go:353\n```go\n// Current:\n_ = m.repo.ReleasePort(ctx, instance.Port)\n\n// Fixed:\n// Release port back to pool. Error ignored because we're already in cleanup\n// path and port will eventually be reclaimed by pool initialization on restart.\n_ = m.repo.ReleasePort(ctx, instance.Port)\n```\n\n### 2. pool/impl.go:367-368\n```go\n// Current:\n_ = m.runtime.Stop(ctx, instance.ID)\n_ = m.repo.ReleasePort(ctx, instance.Port)\n\n// Fixed:\n// Best-effort cleanup during provision failure. Container may not exist yet,\n// and port release failure is acceptable since pool reinitializes on startup.\n_ = m.runtime.Stop(ctx, instance.ID)\n_ = m.repo.ReleasePort(ctx, instance.Port)\n```\n\n### 3. pool/impl.go:381-382\n```go\n// Similar pattern - add explanatory comment\n```\n\n### 4. api/handler.go:231\n```go\n// Current:\n_ = h.store.SaveInstance(ctx, instance)\n\n// Fixed:\n// Save updated instance with user IP. Error ignored because instance is\n// already successfully acquired and functional. Logging happens on failure.\nif err := h.store.SaveInstance(ctx, instance); err != nil {\n    h.logger.Warn(\"Failed to save instance with user IP\", \"id\", instance.ID, \"error\", err)\n}\n```\n\n### 5. docker.go:126\n```go\n// Current:\n_ = r.client.ContainerRemove(ctx, resp.ID, ...)\n\n// Fixed:\n// Clean up container on creation failure. Error ignored because container\n// may not have been fully created. Orphaned containers cleaned by external process.\n_ = r.client.ContainerRemove(ctx, resp.ID, container.RemoveOptions{Force: true})\n```\n\n### 6. main.go godotenv\n```go\n// Current:\n_ = godotenv.Load()\n\n// Fixed:\n// Load .env file if present. Error ignored because .env is optional -\n// production uses real environment variables.\n_ = godotenv.Load()\n```\n\n## Implementation Steps\n1. For each location, add a comment explaining WHY the error is ignored\n2. Consider if any should actually log warnings instead of pure suppression\n3. handler.go:231 should probably log a warning, not fully suppress\n4. Ensure comments are concise but complete\n\n## Verification\n1. Run `make test` - all tests pass\n2. `grep -r \"_ =\" internal/ cmd/` shows all suppressions have preceding comments\n3. Code review confirms all suppressions are intentional\n\n## Dependencies\nNone","status":"open","priority":3,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:08.294554+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:31.054395+04:00","comments":[{"id":8,"issue_id":"demo-multi-plexer-339","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-339'\n\n2. **Report**: bd comments add demo-multi-plexer-339 '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-339 --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments add \u003cnext-id\u003e '## Kickoff: Previous=demo-multi-plexer-339, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:48Z"}]}
{"id":"demo-multi-plexer-46z","title":"Container health check fails for nginx:alpine in Docker","description":"During E2E testing, discovered that nginx:alpine containers start but fail the HealthCheck in pool/impl.go. Container shows as 'unhealthy' in docker ps. HTTP requests to the mapped port (e.g., localhost:32919) fail with connection refused.\n\nSymptoms:\n- Container starts successfully\n- Port mapping shows in 'docker ps' (0.0.0.0:32919-\u003e80/tcp)\n- curl to port returns connection refused\n- Pool replenisher logs: 'container failed health check: timeout waiting for container to be ready'\n\nInvestigation needed:\n1. Check if nginx is actually starting inside container\n2. Verify Docker network configuration (deployments_demo-net)\n3. Check if port binding is working correctly\n4. Review HealthCheck implementation in internal/container/docker.go:200","status":"closed","priority":1,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T17:37:29.620068+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T17:52:12.050129+04:00","closed_at":"2026-01-15T17:52:12.050129+04:00","close_reason":"Fixed health check race condition: Added TCP port check before HTTP (catches 'connection refused' faster), added diagnostic logging, reduced polling interval from 2s to 1s, added 1s initial delay (start period). Tests pass with container becoming healthy in 1-2 attempts."}
{"id":"demo-multi-plexer-4of","title":"Implement NATS Queue","description":"Implement Publisher and Consumer interfaces in internal/queue/nats.go. Use JetStream for durability. Wire in cmd/server/main.go. Config already exists in cfg.Queue.","status":"closed","priority":2,"issue_type":"feature","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T16:45:17.046666+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T22:22:45.543872+04:00","closed_at":"2026-01-15T22:22:45.543872+04:00","close_reason":"Implemented NATS JetStream Publisher and Consumer with TDD, wired into main.go with stub handlers"}
{"id":"demo-multi-plexer-5fp","title":"Phase 5: Integration Tests","description":"Create integration tests:\n- internal/database/prestashop_test.go - real DB tests\n- Test UpdateDomain with actual table updates\n- Test DropPrefixedTables cleanup\n- Test pool manager with CRIU restore (if checkpoint available)\n\nRun with: make test\nEnsure make lint passes.","status":"closed","priority":3,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T00:30:24.429886+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T00:45:51.258961+04:00","closed_at":"2026-01-16T00:45:51.258961+04:00","close_reason":"All 113 tests pass. Database integration tests verify UpdateDomain, ClearCaches, DropPrefixedTables. CRIU tests exist in container packages.","dependencies":[{"issue_id":"demo-multi-plexer-5fp","depends_on_id":"demo-multi-plexer-s7d","type":"blocks","created_at":"2026-01-16T00:30:39.684929+04:00","created_by":"Edgar I."}]}
{"id":"demo-multi-plexer-5l9","title":"Implement Podman + CRIU Runtime","description":"Implement Runtime interface in internal/container/podman.go. Methods: RestoreFromCheckpoint, Start, Stop, Inspect, HealthCheck. Wire mode detection in cmd/server/main.go. Target: 50-200ms restore from checkpoint.","status":"closed","priority":1,"issue_type":"feature","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T16:45:15.869856+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T19:54:22.855662+04:00","closed_at":"2026-01-15T19:54:22.855662+04:00","close_reason":"Podman runtime with CRIU checkpoint/restore implemented. All tests pass. Committed as 140964c."}
{"id":"demo-multi-plexer-5qz","title":"Phase 2: Pool Manager Integration","description":"Modify internal/pool/impl.go:\n1. Add psDB *database.PrestaShopDB field to PoolManager struct\n2. Update NewPoolManager() to accept PrestaShopDB parameter\n3. In provisionInstance(): call UpdateDomain() + ClearCaches() after CRIU restore\n4. In Release(): call DropPrefixedTables() before container stop\n\nMust maintain backward compatibility - psDB can be nil for non-CRIU mode.","status":"closed","priority":2,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T00:30:20.919402+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T00:36:56.023111+04:00","closed_at":"2026-01-16T00:36:56.023111+04:00","close_reason":"Pool manager integrated with PrestaShopDB. All 105 tests pass.","dependencies":[{"issue_id":"demo-multi-plexer-5qz","depends_on_id":"demo-multi-plexer-egr","type":"blocks","created_at":"2026-01-16T00:30:37.214943+04:00","created_by":"Edgar I."}]}
{"id":"demo-multi-plexer-6a1","title":"Phase 2: Production Readiness","status":"open","priority":1,"issue_type":"epic","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T16:45:01.62418+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T16:45:01.62418+04:00","dependencies":[{"issue_id":"demo-multi-plexer-6a1","depends_on_id":"demo-multi-plexer-5l9","type":"blocks","created_at":"2026-01-15T16:45:39.011708+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-6a1","depends_on_id":"demo-multi-plexer-4of","type":"blocks","created_at":"2026-01-15T16:45:39.061366+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-6a1","depends_on_id":"demo-multi-plexer-nlq","type":"blocks","created_at":"2026-01-15T16:45:39.108334+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-6a1","depends_on_id":"demo-multi-plexer-x9q","type":"blocks","created_at":"2026-01-15T16:45:39.1555+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-6a1","depends_on_id":"demo-multi-plexer-8cv","type":"blocks","created_at":"2026-01-15T16:45:39.203212+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-6a1","depends_on_id":"demo-multi-plexer-8pj","type":"blocks","created_at":"2026-01-15T16:45:39.257302+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-6a1","depends_on_id":"demo-multi-plexer-zan","type":"blocks","created_at":"2026-01-16T01:27:05.671174+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-6a1","depends_on_id":"demo-multi-plexer-xwe","type":"blocks","created_at":"2026-01-16T01:27:05.721746+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-6a1","depends_on_id":"demo-multi-plexer-0ga","type":"blocks","created_at":"2026-01-16T01:27:05.771082+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-6a1","depends_on_id":"demo-multi-plexer-hoh","type":"blocks","created_at":"2026-01-16T01:27:05.820006+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-6a1","depends_on_id":"demo-multi-plexer-8k0","type":"blocks","created_at":"2026-01-16T01:27:05.869236+04:00","created_by":"Edgar I."}]}
{"id":"demo-multi-plexer-7hc","title":"Implement Caddy Route Manager","description":"Implement internal/proxy/caddy.go:\n- HTTP client for Caddy admin API (localhost:2019)\n- POST routes dynamically via /config/apps/http/servers/srv0/routes\n- DELETE routes when instances expire\n- Write comprehensive tests","status":"closed","priority":1,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T15:08:31.461959+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T15:21:39.150106+04:00","closed_at":"2026-01-15T15:21:39.150106+04:00","close_reason":"Implemented CaddyRouteManager with Add/Remove/Get/List routes. Unit and mock tests passing."}
{"id":"demo-multi-plexer-7jd","title":"Implement Valkey Repository","description":"Implement internal/store/valkey.go:\n- Connect to Valkey using github.com/valkey-io/valkey-go\n- Implement all Repository interface methods\n- Use LPUSH/LPOP for atomic pool operations\n- Enable keyspace notifications for TTL expiry\n- Write comprehensive tests","status":"closed","priority":1,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T15:08:28.737839+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T15:14:13.436354+04:00","closed_at":"2026-01-15T15:14:13.436354+04:00","close_reason":"Implemented ValkeyRepository with all 17 interface methods, comprehensive tests added"}
{"id":"demo-multi-plexer-8cv","title":"Implement Real Prometheus Metrics","description":"Integrate github.com/prometheus/client_golang. Track: pool size, acquisition latency, container startup time, route operations. Fix placeholder in internal/api/handler.go.","status":"closed","priority":2,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T16:45:21.739296+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T23:22:06.921666+04:00","closed_at":"2026-01-15T23:22:06.921666+04:00","close_reason":"Implemented real Prometheus metrics with github.com/prometheus/client_golang. Added metrics for pool gauges, acquisition counters/latency, provisioning duration, health check timing, and route operations."}
{"id":"demo-multi-plexer-8k0","title":"Switch instance IDs from timestamp format to UUID","description":"## Problem\nInstance IDs use predictable timestamp+counter format: `demo-1737074312-5`. This enables:\n- ID enumeration attacks\n- Predictable next ID guessing\n- Information leakage (creation time visible)\n\n## Security Impact\nEnables IDOR (Insecure Direct Object Reference) attacks when combined with lack of authentication.\n\n## Files to Modify\n- `internal/container/docker.go` - Lines 291-296: `generateInstanceID()`\n- `internal/container/docker_test.go` - Lines 271-282: Update test\n- `internal/pool/impl.go` - Lines 467-477: `generateHostname()` and `generateDBPrefix()`\n- `internal/pool/impl_test.go` - Lines 547-566: Update tests\n- `go.mod` - Promote google/uuid from indirect to direct dependency\n\n## Current Code\n```go\n// internal/container/docker.go:291-296\nfunc generateInstanceID() string {\n    n := atomic.AddUint64(\u0026instanceIDCounter, 1)\n    return fmt.Sprintf(\"demo-%d-%d\", time.Now().Unix(), n)\n}\n```\n\n## New Code\n```go\nimport \"github.com/google/uuid\"\n\nfunc generateInstanceID() string {\n    return fmt.Sprintf(\"demo-%s\", uuid.New().String())\n}\n```\n\n## Implementation Steps\n1. Add `\"github.com/google/uuid\"` import to docker.go\n2. Replace generateInstanceID() body with UUID generation\n3. Remove `instanceIDCounter` atomic variable (no longer needed)\n4. Update docker_test.go to validate UUID format instead of checking length\n5. Update generateHostname() in pool/impl.go similarly\n6. Update generateDBPrefix() to use truncated UUID (8 chars for DB compatibility):\n   ```go\n   func (m *PoolManager) generateDBPrefix() string {\n       id := uuid.New().String()\n       return fmt.Sprintf(\"d%s_\", strings.ReplaceAll(id[:8], \"-\", \"\"))\n   }\n   ```\n7. Update pool/impl_test.go tests for new format\n8. Run `go mod tidy` to update dependencies\n\n## Verification\n1. Run `make test` - all tests pass\n2. Start server: `make dev`\n3. Acquire instance and verify ID format: `demo-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`\n4. Verify DB prefix format: `dxxxxxxxx_` (8 hex chars)\n5. Verify hostname format matches instance ID pattern\n6. Check go.mod shows google/uuid as direct dependency\n\n## Dependencies\nNone - google/uuid already exists as indirect dependency","status":"open","priority":2,"issue_type":"feature","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:13.481409+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:09.353169+04:00","comments":[{"id":10,"issue_id":"demo-multi-plexer-8k0","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-8k0'\n\n2. **Report**: bd comments demo-multi-plexer-8k0 --add '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-8k0 --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments \u003cnext-id\u003e --add '## Kickoff: Previous=demo-multi-plexer-8k0, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:49Z"}]}
{"id":"demo-multi-plexer-8nz","title":"Implement Pool Manager","description":"Implement internal/pool/impl.go:\n- Implement Manager interface\n- Wire up Repository, Runtime, RouteManager\n- Background replenishment loop with watermark triggers\n- Stats reporting\n- Write comprehensive tests","status":"closed","priority":2,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T15:08:32.538553+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T15:24:56.896229+04:00","closed_at":"2026-01-15T15:24:56.896229+04:00","close_reason":"Implemented PoolManager with Acquire, Release, Stats, Replenisher. All tests passing.","dependencies":[{"issue_id":"demo-multi-plexer-8nz","depends_on_id":"demo-multi-plexer-7jd","type":"blocks","created_at":"2026-01-15T15:08:41.737465+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-8nz","depends_on_id":"demo-multi-plexer-v51","type":"blocks","created_at":"2026-01-15T15:08:41.785528+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-8nz","depends_on_id":"demo-multi-plexer-7hc","type":"blocks","created_at":"2026-01-15T15:08:41.833733+04:00","created_by":"Edgar I."}]}
{"id":"demo-multi-plexer-8pj","title":"Production Validation Fixes","description":"Add TTL to rate limit counters in internal/store/valkey.go. Validate checkpoint path exists when Podman mode enabled. Switch instance IDs from timestamp to UUID.","status":"closed","priority":3,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T16:45:22.729296+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:27:12.143703+04:00","closed_at":"2026-01-16T01:27:12.143703+04:00","close_reason":"Expanded into 17 comprehensive production readiness issues from full codebase audit"}
{"id":"demo-multi-plexer-989","title":"Create PrestaShop CRIU checkpoint for fast restore","description":"Create a PrestaShop checkpoint image for CRIU fast restore provisioning.\n\n## Background\nCRIU checkpoint/restore works on macOS Podman (verified with busybox container). However, complex containers with network sockets (like nginx) may fail due to VM limitations.\n\n## Tasks\n1. Set up PrestaShop container on a Linux host (native, not macOS VM)\n2. Wait for PrestaShop to fully initialize\n3. Create CRIU checkpoint: `podman container checkpoint prestashop --export=/path/to/checkpoint.tar.gz`\n4. Test restore and verify PrestaShop works\n5. Document the checkpoint creation process\n\n## Technical Notes\n- CRIU requires root: use `sudo podman checkpoint`\n- macOS Podman VM has socket limitations - may need native Linux for PrestaShop\n- Checkpoint file should be placed at CONTAINER_CHECKPOINT_PATH env var location\n- Tested busybox checkpoint works (47KB, /Users/boss/dev/demo-multi-plexer/checkpoints/busybox-test.tar.gz)\n\n## Acceptance Criteria\n- PrestaShop checkpoint file created and tested\n- Restore completes in \u003c500ms\n- PrestaShop responds to HTTP requests after restore","status":"in_progress","priority":2,"issue_type":"feature","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T22:03:12.38094+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T00:19:04.24209+04:00","dependencies":[{"issue_id":"demo-multi-plexer-989","depends_on_id":"demo-multi-plexer-egr","type":"blocks","created_at":"2026-01-16T00:30:41.390411+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-989","depends_on_id":"demo-multi-plexer-5qz","type":"blocks","created_at":"2026-01-16T00:30:41.438991+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-989","depends_on_id":"demo-multi-plexer-s7d","type":"blocks","created_at":"2026-01-16T00:30:41.487464+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-989","depends_on_id":"demo-multi-plexer-z89","type":"blocks","created_at":"2026-01-16T00:30:41.536329+04:00","created_by":"Edgar I."},{"issue_id":"demo-multi-plexer-989","depends_on_id":"demo-multi-plexer-5fp","type":"blocks","created_at":"2026-01-16T00:30:41.584864+04:00","created_by":"Edgar I."}],"comments":[{"id":1,"issue_id":"demo-multi-plexer-989","author":"Edgar I.","text":"## Research Complete - Implementation Plan Ready\n\n### Research Documents Created\n- `/research/prestashop-flashlight-deep-dive.md` - Comprehensive 400+ line guide covering:\n  - Complete environment variable reference\n  - 4 methods for instant domain configuration\n  - Module installation patterns\n  - CRIU checkpoint timing and post-restore steps\n  - Shared MariaDB multi-tenant architecture\n  - Database cleanup SQL patterns\n\n### Key Technical Findings\n\n**PrestaShop Flashlight Configuration:**\n- Image: `prestashop/prestashop-flashlight:9.0.0`\n- Key env vars: `PS_DOMAIN`, `DB_PREFIX`, `MYSQL_HOST`, `INSTALL_MODULES_DIR`\n- Does NOT include MySQL - requires external MariaDB\n- 15-30s cold start, 50-200ms CRIU restore expected\n\n**Critical Implementation Details:**\n1. `PS_DOMAIN` must be updated in DB after CRIU restore - Tables: `{prefix}shop_url` and `{prefix}configuration`\n2. `DB_PREFIX` provides multi-tenant isolation (e.g., `demo_abc123_`)\n3. Modules installed at checkpoint time via `INSTALL_MODULES_DIR`\n4. TCP connections auto-reconnect - PrestaShop handles gracefully\n5. Cache clearing essential post-restore\n\n### Three Implementation Gaps Identified\n\n**Gap 1:** Checkpoint Creation Workflow - Create `internal/checkpoint/` package\n**Gap 2:** Post-Restore Domain Config - Create `internal/database/prestashop.go`\n**Gap 3:** Database Cleanup on TTL - Add `DropPrefixedTables()` method\n\n### Implementation Phases\n- Phase 1: Database Package (Day 1)\n- Phase 2: Post-Restore Integration (Day 1-2)\n- Phase 3: Cleanup Integration (Day 2)\n- Phase 4: Checkpoint Creator (Day 3)\n- Phase 5: Integration Testing (Day 3-4)\n\nSee handoff document at `/research/criu-implementation-handoff.md` for complete implementation instructions.","created_at":"2026-01-15T20:12:53Z"},{"id":2,"issue_id":"demo-multi-plexer-989","author":"Edgar I.","text":"## Session Start Instructions\n\n**ALWAYS do these before implementation:**\n\n1. Read research files:\n   - `research/second-research.md` - Architecture overview\n   - `research/prestashop-flashlight-deep-dive.md` - PrestaShop config details\n   - `research/criu-implementation-handoff.md` - Implementation plan\n\n2. Run 2 Explore agents to prime codebase context:\n   - Explore pool manager and container runtime patterns\n   - Explore config, store, and test patterns\n\n3. Delegate validation tasks to sub-agents (keep main context clean)\n\n4. After completing work: validate, update beads, craft next session kickoff","created_at":"2026-01-15T20:30:00Z"},{"id":3,"issue_id":"demo-multi-plexer-989","author":"Edgar I.","text":"## Implementation Complete - Ready for Checkpoint Creation\n\n### Completed Work (This Session)\n\n**Phase 1: Database Package** ✅\n- Created `internal/database/prestashop.go`\n- Methods: `UpdateDomain()`, `ClearCaches()`, `DropPrefixedTables()`, `Ping()`\n- 5 integration tests passing against real MariaDB\n\n**Phase 2: Pool Manager Integration** ✅\n- Added `psDB` field to PoolManager\n- Updated `NewPoolManager()` signature\n- Post-CRIU restore: calls `UpdateDomain()` + `ClearCaches()`\n- Release cleanup: calls `DropPrefixedTables()`\n\n**Phase 3: Server Wiring** ✅\n- PrestaShopDB initialized at startup\n- Graceful fallback if DB unavailable\n- Passed to pool manager\n\n**Phase 4: Checkpoint Creator Tool** ✅\n- Created `cmd/checkpoint/main.go`\n- CLI flags for all configuration options\n- Steps: start container, wait for health, warm caches, checkpoint, cleanup\n\n**Phase 5: Integration Tests** ✅\n- Database tests: UpdateDomain, ClearCaches, DropPrefixedTables\n- Container tests: CRIU availability, error handling\n- All 113 tests passing\n\n### Remaining Work\n\n**Create actual checkpoint on Linux host:**\n```bash\n# On a Linux server with Podman + CRIU:\ngo build -o checkpoint ./cmd/checkpoint\n./checkpoint --output=/var/lib/checkpoints/prestashop.tar.gz\n```\n\n### Files Created/Modified\n- `internal/database/prestashop.go` (new)\n- `internal/database/prestashop_test.go` (new)\n- `internal/pool/impl.go` (modified)\n- `cmd/server/main.go` (modified)\n- `cmd/checkpoint/main.go` (new)","created_at":"2026-01-15T20:46:13Z"}]}
{"id":"demo-multi-plexer-9az","title":"Standardize API response formats with typed structs","description":"## Problem\nAPI responses mix `gin.H{}` inline maps with typed response structs. This creates inconsistency for API consumers.\n\n## Current State\n| Endpoint | Response Type | Issue |\n|----------|---------------|-------|\n| GET /health | gin.H | Should be typed |\n| POST /acquire | AcquireResponse | OK |\n| GET /:id | InstanceResponse | OK |\n| POST /:id/extend | gin.H | Should be typed |\n| DELETE /:id | gin.H | Should be typed |\n| GET /pool/stats | gin.H | Should be typed |\n\n## File to Modify\n- `internal/api/handler.go` - Add typed response structs and use them\n\n## Proposed Response Structs\n```go\n// Add to handler.go or create types.go\n\n// HealthResponse for GET /health\ntype HealthResponse struct {\n    Status string `json:\"status\"`\n}\n\n// ExtendResponse for POST /demo/:id/extend\ntype ExtendResponse struct {\n    ID        string    `json:\"id\"`\n    ExpiresAt time.Time `json:\"expires_at\"`\n    TTL       int64     `json:\"ttl\"`\n}\n\n// DeleteResponse for DELETE /demo/:id\ntype DeleteResponse struct {\n    Message string `json:\"message\"`\n}\n\n// PoolStatsResponse for GET /pool/stats\ntype PoolStatsResponse struct {\n    Ready       int64 `json:\"ready\"`\n    Warming     int64 `json:\"warming\"`\n    Assigned    int64 `json:\"assigned\"`\n    TotalServed int64 `json:\"total_served\"`\n}\n```\n\n## Implementation Steps\n1. Define typed response structs (HealthResponse, ExtendResponse, DeleteResponse, PoolStatsResponse)\n2. Update health handler to use HealthResponse\n3. Update extendDemo handler to use ExtendResponse\n4. Update releaseDemo handler to use DeleteResponse\n5. Update getPoolStats handler to use PoolStatsResponse\n6. Ensure all error responses use existing ErrorResponse struct\n\n## Verification\n1. Run `make test` - all tests pass\n2. Verify API responses are consistently formatted\n3. Test each endpoint and confirm JSON structure matches struct definitions\n4. Update any API documentation if it exists\n\n## Dependencies\nNone","status":"open","priority":3,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:11.343105+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:34.39605+04:00","comments":[{"id":12,"issue_id":"demo-multi-plexer-9az","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-9az'\n\n2. **Report**: bd comments add demo-multi-plexer-9az '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-9az --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments add \u003cnext-id\u003e '## Kickoff: Previous=demo-multi-plexer-9az, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:51Z"}]}
{"id":"demo-multi-plexer-agk","title":"Split Repository interface into focused interfaces (ISP)","description":"## Problem\nThe Repository interface in `internal/store/repository.go` contains 19 methods serving 8 different concerns. This violates Interface Segregation Principle (ISP).\n\n## Impact\n- Clients forced to depend on methods they don't use\n- Testing requires mocking all 19 methods\n- Interface too large to reason about\n\n## Files to Modify\n- `internal/store/repository.go` - Split interface\n- `internal/store/valkey.go` - Ensure implements all interfaces\n- `internal/api/handler.go` - Update type references\n- `internal/pool/impl.go` - Update type references\n\n## Current Code (repository.go)\n```go\ntype Repository interface {\n    // Instance operations (5 methods)\n    SaveInstance(ctx context.Context, instance *domain.Instance) error\n    GetInstance(ctx context.Context, id string) (*domain.Instance, error)\n    DeleteInstance(ctx context.Context, id string) error\n    UpdateInstanceState(ctx context.Context, id string, state domain.InstanceState) error\n    ListInstances(ctx context.Context) ([]*domain.Instance, error)\n    \n    // Pool operations (3 methods)\n    AddToPool(ctx context.Context, instanceID string) error\n    AcquireFromPool(ctx context.Context) (*domain.Instance, error)\n    GetPoolSize(ctx context.Context) (int, error)\n    \n    // TTL operations (3 methods)\n    SetInstanceTTL(ctx context.Context, id string, ttl time.Duration) error\n    ExtendInstanceTTL(ctx context.Context, id string, extension time.Duration) error\n    GetExpiredInstances(ctx context.Context) ([]*domain.Instance, error)\n    \n    // Port allocation (2 methods)\n    AllocatePort(ctx context.Context) (int, error)\n    ReleasePort(ctx context.Context, port int) error\n    \n    // Rate limiting (2 methods)\n    CheckRateLimit(ctx context.Context, ip string, hourlyLimit, dailyLimit int) (bool, error)\n    IncrementRateLimit(ctx context.Context, ip string) error\n    \n    // Statistics (1 method)\n    GetPoolStats(ctx context.Context) (*domain.PoolStats, error)\n    \n    // Health (1 method)\n    Ping(ctx context.Context) error\n    \n    // Lifecycle (1 method)\n    Close() error\n}\n```\n\n## Proposed Split\n```go\n// InstanceStore handles instance CRUD\ntype InstanceStore interface {\n    SaveInstance(ctx context.Context, instance *domain.Instance) error\n    GetInstance(ctx context.Context, id string) (*domain.Instance, error)\n    DeleteInstance(ctx context.Context, id string) error\n    UpdateInstanceState(ctx context.Context, id string, state domain.InstanceState) error\n    ListInstances(ctx context.Context) ([]*domain.Instance, error)\n}\n\n// PoolStore handles pool operations\ntype PoolStore interface {\n    AddToPool(ctx context.Context, instanceID string) error\n    AcquireFromPool(ctx context.Context) (*domain.Instance, error)\n    GetPoolSize(ctx context.Context) (int, error)\n    SetInstanceTTL(ctx context.Context, id string, ttl time.Duration) error\n    ExtendInstanceTTL(ctx context.Context, id string, extension time.Duration) error\n    GetExpiredInstances(ctx context.Context) ([]*domain.Instance, error)\n}\n\n// PortAllocator handles port management\ntype PortAllocator interface {\n    AllocatePort(ctx context.Context) (int, error)\n    ReleasePort(ctx context.Context, port int) error\n}\n\n// RateLimiter handles rate limiting\ntype RateLimiter interface {\n    CheckRateLimit(ctx context.Context, ip string, hourlyLimit, dailyLimit int) (bool, error)\n    IncrementRateLimit(ctx context.Context, ip string) error\n}\n\n// Repository composes all interfaces (for backward compatibility)\ntype Repository interface {\n    InstanceStore\n    PoolStore\n    PortAllocator\n    RateLimiter\n    GetPoolStats(ctx context.Context) (*domain.PoolStats, error)\n    Ping(ctx context.Context) error\n    Close() error\n}\n```\n\n## Implementation Steps\n1. Define new focused interfaces in repository.go\n2. Keep Repository as composition of smaller interfaces (backward compatible)\n3. Update ValkeyRepository compile-time checks to verify all interfaces\n4. Gradually update consumers to use focused interfaces where possible\n5. Update mock implementations in test files\n\n## Verification\n1. Run `make test` - all tests pass\n2. Verify compile-time interface checks: `var _ InstanceStore = (*ValkeyRepository)(nil)`\n3. Code should compile without changes to consumers (composition maintains compatibility)\n\n## Dependencies\nNone - backward compatible change","status":"open","priority":2,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:12.340842+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:11.750394+04:00","comments":[{"id":20,"issue_id":"demo-multi-plexer-agk","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-agk'\n\n2. **Report**: bd comments demo-multi-plexer-agk --add '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-agk --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments \u003cnext-id\u003e --add '## Kickoff: Previous=demo-multi-plexer-agk, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:58Z"}]}
{"id":"demo-multi-plexer-avw","title":"Mask sensitive data (passwords) in log output","description":"## Problem\nDatabase passwords and other sensitive data could leak to logs through environment variable logging or error messages.\n\n## Risk Areas\n- `internal/container/docker.go` buildEnvVars() - Creates env with DB_PASSWD\n- `internal/container/podman.go` buildEnvVars() - Same pattern\n- Any debug logging of configuration\n\n## Files to Modify\n- `internal/container/docker.go` - Lines 267-289\n- `internal/container/podman.go` - Lines 360-382\n- `pkg/logging/logger.go` - Optional: Add sanitization helper\n\n## Current Risk\n```go\n// If these env vars are ever logged, password is exposed:\nfmt.Sprintf(\"DB_PASSWD=%s\", r.psCfg.DBPassword),\n```\n\n## Mitigation Options\n\n### Option A: Never log environment variables (Recommended)\nEnsure buildEnvVars() output is never logged. Add comment:\n```go\n// buildEnvVars creates container environment variables.\n// WARNING: Contains sensitive data (passwords). Never log this output.\nfunc (r *DockerRuntime) buildEnvVars(opts StartOptions) []string {\n```\n\n### Option B: Create sanitized version for logging\n```go\n// SanitizeEnv creates a copy with passwords masked for safe logging\nfunc SanitizeEnv(env []string) []string {\n    sanitized := make([]string, len(env))\n    sensitiveKeys := []string{\"PASSWD\", \"PASSWORD\", \"SECRET\", \"KEY\", \"TOKEN\"}\n    \n    for i, e := range env {\n        for _, key := range sensitiveKeys {\n            if strings.Contains(strings.ToUpper(e), key) {\n                parts := strings.SplitN(e, \"=\", 2)\n                if len(parts) == 2 {\n                    sanitized[i] = parts[0] + \"=***MASKED***\"\n                    continue\n                }\n            }\n        }\n        sanitized[i] = e\n    }\n    return sanitized\n}\n```\n\n## Implementation Steps\n1. Audit all logging calls that might include environment variables\n2. Add WARNING comments to buildEnvVars() functions\n3. If any logging exists, use sanitization\n4. Consider adding slog hook that auto-sanitizes known sensitive fields\n5. Review config.go for any password logging on startup\n\n## Verification\n1. Run `make test` - all tests pass\n2. Start server with `LOG_LEVEL=debug`\n3. Provision container and grep logs for password: `grep -i \"passwd\\|password\" logs`\n4. Verify no actual passwords appear in output\n\n## Dependencies\nNone","status":"open","priority":3,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:13.727088+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:36.952904+04:00","comments":[{"id":16,"issue_id":"demo-multi-plexer-avw","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-avw'\n\n2. **Report**: bd comments add demo-multi-plexer-avw '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-avw --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments add \u003cnext-id\u003e '## Kickoff: Previous=demo-multi-plexer-avw, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:54Z"}]}
{"id":"demo-multi-plexer-bht","title":"Fix nil pointer dereference in extendDemo handler","description":"## Problem\nIn `internal/api/handler.go:340`, error from GetInstance is discarded with blank identifier. If GetInstance fails, `instance` is nil and subsequent access causes panic.\n\n## Impact\nServer crash on Valkey failure during TTL extension.\n\n## File to Modify\n- `internal/api/handler.go` - Lines 339-345\n\n## Current Code (BROKEN)\n```go\n// Line 339-345\ninstance, _ = h.store.GetInstance(ctx, id)  // ← Error ignored!\n\nc.JSON(http.StatusOK, gin.H{\n    \"id\":         instance.ID,        // ← PANIC if instance is nil\n    \"expires_at\": instance.ExpiresAt,\n    \"ttl\":        instance.TTL(),\n})\n```\n\n## Fixed Code\n```go\n// Get updated instance\ninstance, err = h.store.GetInstance(ctx, id)\nif err != nil {\n    h.logger.Error(\"Failed to get instance after TTL extension\", \"id\", id, \"error\", err)\n    c.JSON(http.StatusInternalServerError, ErrorResponse{\n        Error: \"failed to retrieve updated instance\",\n        Code:  \"INTERNAL_ERROR\",\n    })\n    return\n}\n\nc.JSON(http.StatusOK, gin.H{\n    \"id\":         instance.ID,\n    \"expires_at\": instance.ExpiresAt,\n    \"ttl\":        instance.TTL(),\n})\n```\n\n## Implementation Steps\n1. Replace `instance, _ = h.store.GetInstance(ctx, id)` with proper error handling\n2. Add error check: if err != nil, return 500 with error response\n3. Log the error with context\n4. Only proceed to JSON response if instance is valid\n\n## Verification\n1. Run `make test` - all tests pass\n2. Add unit test that mocks GetInstance failure after ExtendInstanceTTL success\n3. Verify server returns 500 instead of crashing\n4. Test manually by stopping Valkey mid-request\n\n## Dependencies\nNone","status":"open","priority":2,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:08.741215+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:07.678907+04:00","comments":[{"id":17,"issue_id":"demo-multi-plexer-bht","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-bht'\n\n2. **Report**: bd comments demo-multi-plexer-bht --add '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-bht --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments \u003cnext-id\u003e --add '## Kickoff: Previous=demo-multi-plexer-bht, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:54Z"}]}
{"id":"demo-multi-plexer-dbr","title":"Add request ID middleware for distributed tracing","description":"## Problem\nNo request ID correlation in logs. Different handlers log independently, making it impossible to trace a single request through the system.\n\n## Impact\n- Debugging production issues requires manual log correlation\n- Cannot track acquire→release lifecycle\n- Support requests harder to investigate\n\n## Files to Modify\n- `internal/api/middleware.go` - Add request ID middleware (create if doesn't exist)\n- `internal/api/handler.go` - Apply middleware, pass request ID to logger\n- `pkg/logging/logger.go` - Optional: helper for request-scoped logger\n\n## Implementation\n\n### 1. Create/Update middleware.go\n```go\npackage api\n\nimport (\n    \"github.com/gin-gonic/gin\"\n    \"github.com/google/uuid\"\n)\n\nconst RequestIDHeader = \"X-Request-ID\"\nconst RequestIDKey = \"request_id\"\n\n// RequestID middleware adds a unique request ID to each request\nfunc RequestID() gin.HandlerFunc {\n    return func(c *gin.Context) {\n        // Use existing header if provided, otherwise generate\n        requestID := c.GetHeader(RequestIDHeader)\n        if requestID == \"\" {\n            requestID = uuid.New().String()\n        }\n        \n        // Store in context and set response header\n        c.Set(RequestIDKey, requestID)\n        c.Header(RequestIDHeader, requestID)\n        \n        c.Next()\n    }\n}\n\n// GetRequestID extracts request ID from gin context\nfunc GetRequestID(c *gin.Context) string {\n    if id, exists := c.Get(RequestIDKey); exists {\n        return id.(string)\n    }\n    return \"\"\n}\n```\n\n### 2. Apply in handler.go\n```go\nfunc (h *Handler) SetupRoutes() *gin.Engine {\n    r := gin.New()\n    r.Use(gin.Recovery())\n    r.Use(RequestID())  // Add request ID first\n    r.Use(h.loggingMiddleware())  // Then logging\n    // ... rest of routes\n}\n```\n\n### 3. Use in handlers\n```go\nfunc (h *Handler) acquireDemo(c *gin.Context) {\n    ctx := c.Request.Context()\n    requestID := GetRequestID(c)\n    logger := h.logger.With(\"request_id\", requestID)\n    \n    logger.Info(\"Acquiring demo instance\")\n    // ... rest of handler using logger\n}\n```\n\n## Implementation Steps\n1. Add RequestID middleware to middleware.go\n2. Apply RequestID middleware in SetupRoutes before other middleware\n3. Update each handler to extract request ID and create scoped logger\n4. Ensure error responses include request ID for client correlation\n5. Update response structs to optionally include request_id field\n\n## Verification\n1. Run `make test` - all tests pass\n2. Start server: `make dev`\n3. Make request: `curl -v http://localhost:8080/api/v1/demo/acquire`\n4. Verify X-Request-ID header in response\n5. Verify logs include request_id field\n6. Make request with custom ID: `curl -H \"X-Request-ID: test-123\" ...`\n7. Verify response uses provided ID\n\n## Dependencies\n- Requires google/uuid (already available from other changes)","status":"open","priority":3,"issue_type":"feature","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:10.24057+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:32.490693+04:00","comments":[{"id":11,"issue_id":"demo-multi-plexer-dbr","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-dbr'\n\n2. **Report**: bd comments add demo-multi-plexer-dbr '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-dbr --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments add \u003cnext-id\u003e '## Kickoff: Previous=demo-multi-plexer-dbr, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:49Z"}]}
{"id":"demo-multi-plexer-egr","title":"Phase 1: Database Package with Real DB Tests","description":"Create internal/database/prestashop.go with:\n- PrestaShopDB struct with sql.DB connection\n- NewPrestaShopDB() constructor\n- UpdateDomain(ctx, dbPrefix, newDomain) - update shop_url and configuration tables\n- ClearCaches(ctx, dbPrefix) - truncate smarty cache tables\n- DropPrefixedTables(ctx, dbPrefix) - cleanup on TTL expiry\n- Ping(ctx) for health checks\n\nTesting: Use REAL MariaDB from docker-compose (make infra-up), NO mocks.\nAdd go-sql-driver/mysql dependency.","status":"closed","priority":2,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T00:30:19.885005+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T00:34:24.575282+04:00","closed_at":"2026-01-16T00:34:24.575282+04:00","close_reason":"Database package created with real DB tests. All 5 tests pass against MariaDB."}
{"id":"demo-multi-plexer-hoh","title":"Replace unbounded replenish goroutine with channel-based trigger","description":"## Problem\nIn `internal/pool/impl.go:115-121`, every `Acquire()` call spawns a new goroutine to trigger replenishment. Under high load, this creates unbounded goroutine accumulation.\n\n## Impact\n- Memory exhaustion under sustained load\n- Goroutines not tracked during shutdown\n- No backpressure mechanism\n- Resource leak over time\n\n## File to Modify\n- `internal/pool/impl.go` - Lines 115-121, plus struct and constructor\n\n## Current Code (BROKEN)\n```go\n// Line 115-121 - Unbounded goroutine spawn\ngo func() {\n    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n    defer cancel()\n    if err := m.TriggerReplenish(ctx); err != nil {\n        m.logger.Warn(\"Replenishment check failed\", \"error\", err)\n    }\n}()\n```\n\n## Fixed Code\n\n1. Add channel to PoolManager struct:\n```go\ntype PoolManager struct {\n    // ... existing fields\n    replenishCh chan struct{}  // Buffered channel for replenish triggers\n}\n```\n\n2. Initialize in NewPoolManager:\n```go\nfunc NewPoolManager(...) *PoolManager {\n    m := \u0026PoolManager{\n        // ... existing fields\n        replenishCh: make(chan struct{}, 1),  // Buffer of 1 for non-blocking send\n    }\n    return m\n}\n```\n\n3. Replace goroutine spawn with non-blocking send:\n```go\n// Trigger async replenishment check (non-blocking)\nselect {\ncase m.replenishCh \u003c- struct{}{}:\n    // Triggered successfully\ndefault:\n    // Already a trigger pending, skip\n}\n```\n\n4. Add dedicated consumer goroutine in StartReplenisher:\n```go\nfunc (m *PoolManager) StartReplenisher(ctx context.Context) {\n    // ... existing replenishLoop goroutine\n    \n    // Add trigger consumer goroutine\n    go func() {\n        for {\n            select {\n            case \u003c-ctx.Done():\n                return\n            case \u003c-m.replenishCh:\n                triggerCtx, cancel := context.WithTimeout(ctx, 30*time.Second)\n                if err := m.TriggerReplenish(triggerCtx); err != nil {\n                    m.logger.Warn(\"Triggered replenishment failed\", \"error\", err)\n                }\n                cancel()\n            }\n        }\n    }()\n}\n```\n\n## Implementation Steps\n1. Add `replenishCh chan struct{}` field to PoolManager struct\n2. Initialize channel in NewPoolManager with buffer size 1\n3. Replace goroutine spawn in Acquire() with non-blocking channel send\n4. Add channel consumer goroutine in StartReplenisher()\n5. Ensure consumer respects context cancellation for clean shutdown\n\n## Verification\n1. Run `make test` - all tests pass\n2. Add test for high-concurrency acquire (100 concurrent requests)\n3. Verify goroutine count doesn't explode with `runtime.NumGoroutine()`\n4. Test graceful shutdown completes within timeout\n5. Load test: `hey -n 1000 -c 100 -m POST http://localhost:8080/api/v1/demo/acquire`\n\n## Dependencies\nNone","status":"open","priority":1,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:12.405962+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:07.815724+04:00","comments":[{"id":9,"issue_id":"demo-multi-plexer-hoh","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m 'fix: replace unbounded goroutine with channel-based trigger - Closes: demo-multi-plexer-hoh'\n\n2. **Report**: bd comments add demo-multi-plexer-hoh '## Done: \u003csummary of changes and files modified\u003e'\n\n3. **Close**: bd close demo-multi-plexer-hoh --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add kickoff context to next issue: bd comments add \u003cnext-id\u003e '## Kickoff: Previous=hoh, Start at=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:49Z"}]}
{"id":"demo-multi-plexer-koz","title":"Protect Caddy admin API from external access","description":"## Problem\nCaddy admin API (port 2019) is exposed in docker-compose.yml. This API provides full control over routing configuration.\n\n## Security Impact\nIf exposed on public interface, attackers can:\n- Modify routes to redirect traffic\n- View all configured routes\n- Disable the proxy entirely\n\n## File to Modify\n- `deployments/docker-compose.yml` - Line 7\n\n## Current Code\n```yaml\n# Line 5-8\nports:\n  - \"80:80\"\n  - \"443:443\"\n  - \"2019:2019\"  # ← Caddy Admin API exposed\n```\n\n## Option A: Remove External Exposure (Recommended)\n```yaml\nports:\n  - \"80:80\"\n  - \"443:443\"\n  # Admin API not exposed externally - access via docker exec or internal network\n```\n\n## Option B: Bind to Localhost Only\n```yaml\nports:\n  - \"80:80\"\n  - \"443:443\"\n  - \"127.0.0.1:2019:2019\"  # Only accessible from localhost\n```\n\n## Implementation Steps\n1. Choose Option A (remove) or Option B (localhost only)\n2. Update docker-compose.yml accordingly\n3. Update CLAUDE.md if it references admin API access\n4. Verify internal/proxy/caddy.go still works (it connects to localhost:2019)\n\n## Verification\n1. Run `make infra-down \u0026\u0026 make infra-up`\n2. Verify Caddy admin API not accessible from external IP\n3. Verify proxy operations still work: `make dev` and acquire instance\n4. If using Option B, verify `curl http://localhost:2019/config/` works locally\n\n## Dependencies\nNone","status":"open","priority":2,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:11.260155+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:10.586372+04:00","comments":[{"id":19,"issue_id":"demo-multi-plexer-koz","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-koz'\n\n2. **Report**: bd comments demo-multi-plexer-koz --add '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-koz --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments \u003cnext-id\u003e --add '## Kickoff: Previous=demo-multi-plexer-koz, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:57Z"}]}
{"id":"demo-multi-plexer-lkg","title":"Wire Up API Handlers","description":"Replace 501 stubs with real implementations:\n- Inject Pool Manager into Handler\n- Implement acquireDemo (POST /demos)\n- Implement getDemo (GET /demos/:id)\n- Implement releaseDemo (DELETE /demos/:id)\n- Implement extendDemo (POST /demos/:id/extend)\n- Implement listDemos (GET /demos)\n- Implement getStats (GET /stats)\n- Write integration tests","status":"closed","priority":2,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T15:08:33.568296+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T15:28:12.389019+04:00","closed_at":"2026-01-15T15:28:12.389019+04:00","close_reason":"All endpoints implemented: acquire, get, extend, release, status, stats, metrics. Tests passing.","dependencies":[{"issue_id":"demo-multi-plexer-lkg","depends_on_id":"demo-multi-plexer-8nz","type":"blocks","created_at":"2026-01-15T15:08:43.005184+04:00","created_by":"Edgar I."}]}
{"id":"demo-multi-plexer-nlq","title":"End-to-End Integration Tests","description":"Create tests/integration/e2e_test.go. Test full flow: acquire → health check → extend TTL → release. Use real containers (Docker first, then Podman). Skip pattern: require E2E_TEST=1 env var.","status":"closed","priority":1,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T16:45:18.421464+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T17:37:24.6357+04:00","closed_at":"2026-01-15T17:37:24.6357+04:00","close_reason":"E2E tests implemented with build tags, table-driven tests, t.Cleanup(). Tests verify full lifecycle but require working pool. Separate health check issue discovered."}
{"id":"demo-multi-plexer-s7d","title":"Phase 3: Server Wiring","description":"Modify cmd/server/main.go:\n1. Add import for internal/database\n2. Add PrestaShop config to internal/config (DB host, port, user, pass, dbname)\n3. Initialize PrestaShopDB after config load\n4. Pass psDB to NewPoolManager()\n5. Add graceful shutdown for DB connection\n\nUpdate .env.example with new PRESTASHOP_DB_* variables.","status":"closed","priority":2,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T00:30:21.953671+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T00:38:16.286957+04:00","closed_at":"2026-01-16T00:38:16.286957+04:00","close_reason":"Server wiring complete. PrestaShopDB initialized and passed to pool manager. All tests pass.","dependencies":[{"issue_id":"demo-multi-plexer-s7d","depends_on_id":"demo-multi-plexer-5qz","type":"blocks","created_at":"2026-01-16T00:30:38.439225+04:00","created_by":"Edgar I."}]}
{"id":"demo-multi-plexer-s8w","title":"Create root README.md with project overview","description":"## Problem\nNo README.md at project root. New team members have no obvious starting point.\n\n## Impact\n- Onboarding friction\n- Project overview scattered across CLAUDE.md\n- GitHub/GitLab preview empty\n\n## File to Create\n- `README.md` - Project root\n\n## Proposed Content\n```markdown\n# PrestaShop Demo Multiplexer\n\nInstant-provisioning system for PrestaShop e-commerce trial instances. Achieves sub-500ms perceived startup using warm pool architecture with pre-warmed containerized instances.\n\n## Quick Start\n\n\\`\\`\\`bash\n# Setup\nmake init              # Copy .env.example → .env\n\n# Start infrastructure\nmake infra-up          # Start Docker Compose services\n\n# Run server\nmake dev               # Run with Docker backend\n\\`\\`\\`\n\n## Architecture\n\n\\`\\`\\`\nHTTP API (Gin)\n     │\n     ├── Pool Manager ─── Container Runtime (Docker/Podman)\n     │         │\n     │         └── Repository (Valkey) ─── State persistence\n     │\n     └── Caddy Route Manager ─── Dynamic reverse proxy\n\\`\\`\\`\n\n**Instant Acquire Flow:**\n1. \\`POST /api/v1/demo/acquire\\` → Pool Manager\n2. Valkey LPOP from warm pool (O(1))\n3. Instance already has Caddy route → Return URL immediately\n\n## API Endpoints\n\n| Method | Endpoint | Description |\n|--------|----------|-------------|\n| POST | /api/v1/demo/acquire | Get instance from pool |\n| GET | /api/v1/demo/:id | Instance details |\n| POST | /api/v1/demo/:id/extend | Extend TTL |\n| DELETE | /api/v1/demo/:id | Release instance |\n| GET | /api/v1/pool/stats | Pool statistics |\n| GET | /health | Health check |\n| GET | /metrics | Prometheus metrics |\n\n## Tech Stack\n\n- **Go 1.25** - Application server\n- **Valkey 8** - State store (Redis-compatible)\n- **Docker/Podman** - Container runtime (CRIU support with Podman)\n- **Caddy 2** - Dynamic reverse proxy\n- **NATS JetStream** - Async message queue\n\n## Development\n\n\\`\\`\\`bash\nmake test              # Run tests\nmake lint              # Run linter\nmake build             # Build binary\nmake clean-all         # Full cleanup\n\\`\\`\\`\n\n## Configuration\n\nSee \\`.env.example\\` for all configuration options. Key settings:\n- \\`POOL_TARGET_SIZE\\` - Number of warm instances\n- \\`CONTAINER_MODE\\` - docker or podman\n- \\`BASE_DOMAIN\\` - Domain for instance URLs\n\n## Documentation\n\n- [CLAUDE.md](./CLAUDE.md) - Detailed project instructions\n- [API Documentation](./docs/api.md) - Full API reference (if exists)\n\n## License\n\n[License information]\n\\`\\`\\`\n\n## Implementation Steps\n1. Create README.md at project root\n2. Include quick start, architecture overview, API summary\n3. Link to CLAUDE.md for detailed instructions\n4. Keep it concise - detailed docs stay in CLAUDE.md\n\n## Verification\n1. README.md exists and renders correctly\n2. Quick start commands work as documented\n3. Links to other docs are valid\n\n## Dependencies\nNone","status":"open","priority":3,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:12.667223+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:35.871737+04:00","comments":[{"id":15,"issue_id":"demo-multi-plexer-s8w","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-s8w'\n\n2. **Report**: bd comments add demo-multi-plexer-s8w '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-s8w --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments add \u003cnext-id\u003e '## Kickoff: Previous=demo-multi-plexer-s8w, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:52Z"}]}
{"id":"demo-multi-plexer-v51","title":"Implement Docker Runtime","description":"Implement internal/container/docker.go:\n- Implement Runtime interface using Docker SDK\n- This is the dev/fallback mode (no CRIU)\n- Use github.com/docker/docker/client\n- Start, Stop, inspect container operations\n- Write comprehensive tests","status":"closed","priority":1,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T15:08:29.911867+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T15:18:10.676087+04:00","closed_at":"2026-01-15T15:18:10.676087+04:00","close_reason":"Implemented DockerRuntime with Start, Stop, Inspect, HealthCheck. Unit tests passing."}
{"id":"demo-multi-plexer-vbr","title":"Fix Docker network configuration for dev mode","description":"When running 'make dev', containers fail to start with:\n'network demo-net not found'\n\nRoot cause: Default network in config.go is 'demo-net' but Docker Compose creates it as 'deployments_demo-net'.\n\nFix: Change default network to empty string in internal/config/config.go line ~99:\n- Network: getEnv(\"CONTAINER_NETWORK\", \"demo-net\"),\n+ Network: getEnv(\"CONTAINER_NETWORK\", \"\"),\n\nEmpty network = use Docker's default bridge, which works for dev mode.","status":"closed","priority":1,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T15:45:17.541724+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T15:48:23.737995+04:00","closed_at":"2026-01-15T15:48:23.737995+04:00","close_reason":"Changed default network from 'demo-net' to empty string. Empty network uses Docker's default bridge, avoiding mismatch with Docker Compose naming."}
{"id":"demo-multi-plexer-wp1","title":"Extract duplicate health check logic to shared function","description":"## Problem\n42 lines of nearly identical TCP + HTTP health check logic duplicated between docker.go and podman.go.\n\n## Impact\n- Code duplication violates DRY\n- Bug fixes must be applied twice\n- Inconsistencies can creep in\n\n## Files to Modify\n- `internal/container/docker.go` - Lines 207-264: waitForReady()\n- `internal/container/podman.go` - Lines 308-357: waitForReady()\n- `internal/container/health.go` - NEW FILE: Shared health check logic\n\n## Current Duplication\nBoth files have nearly identical code:\n```go\nfunc (r *DockerRuntime) waitForReady(ctx context.Context, port int) error {\n    // TCP dial check\n    // HTTP health check with retries\n    // 1 second polling interval\n    // 2 minute timeout\n}\n```\n\n## Proposed Shared Implementation\nCreate `internal/container/health.go`:\n```go\npackage container\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"net\"\n    \"net/http\"\n    \"time\"\n)\n\n// HealthChecker provides container health check functionality\ntype HealthChecker struct {\n    httpClient  *http.Client\n    tcpTimeout  time.Duration\n    httpTimeout time.Duration\n    interval    time.Duration\n    maxWait     time.Duration\n    logger      *slog.Logger\n}\n\n// NewHealthChecker creates a new health checker with defaults\nfunc NewHealthChecker(httpClient *http.Client, logger *slog.Logger) *HealthChecker {\n    return \u0026HealthChecker{\n        httpClient:  httpClient,\n        tcpTimeout:  2 * time.Second,\n        httpTimeout: 5 * time.Second,\n        interval:    1 * time.Second,\n        maxWait:     2 * time.Minute,\n        logger:      logger,\n    }\n}\n\n// WaitForReady waits for container to be ready via TCP and HTTP checks\nfunc (h *HealthChecker) WaitForReady(ctx context.Context, port int) error {\n    addr := fmt.Sprintf(\"127.0.0.1:%d\", port)\n    url := fmt.Sprintf(\"http://%s/\", addr)\n    \n    deadline := time.Now().Add(h.maxWait)\n    ticker := time.NewTicker(h.interval)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case \u003c-ctx.Done():\n            return ctx.Err()\n        case \u003c-ticker.C:\n            if time.Now().After(deadline) {\n                return fmt.Errorf(\"timeout waiting for container on port %d\", port)\n            }\n            \n            // TCP check\n            conn, err := net.DialTimeout(\"tcp\", addr, h.tcpTimeout)\n            if err != nil {\n                h.logger.Debug(\"TCP check failed\", \"port\", port, \"error\", err)\n                continue\n            }\n            conn.Close()\n            \n            // HTTP check\n            resp, err := h.httpClient.Get(url)\n            if err != nil {\n                h.logger.Debug(\"HTTP check failed\", \"port\", port, \"error\", err)\n                continue\n            }\n            resp.Body.Close()\n            \n            if resp.StatusCode \u003e= 200 \u0026\u0026 resp.StatusCode \u003c 400 {\n                h.logger.Debug(\"Container ready\", \"port\", port)\n                return nil\n            }\n        }\n    }\n}\n```\n\n## Implementation Steps\n1. Create `internal/container/health.go` with shared HealthChecker\n2. Add `healthChecker *HealthChecker` field to DockerRuntime struct\n3. Initialize healthChecker in NewDockerRuntime using the shared httpClient\n4. Replace waitForReady() body with call to `r.healthChecker.WaitForReady(ctx, port)`\n5. Repeat steps 2-4 for PodmanRuntime\n6. Remove duplicate waitForReady implementations (keep as thin wrappers if needed)\n\n## Verification\n1. Run `make test` - all tests pass\n2. Start server: `make dev`\n3. Provision container and verify health check works\n4. Check logs show health check debug messages\n5. Verify both Docker and Podman paths use shared code\n\n## Dependencies\nDepends on: Create shared HTTP client for health checks (should be done first)","status":"open","priority":3,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:06.348834+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:28.86846+04:00","dependencies":[{"issue_id":"demo-multi-plexer-wp1","depends_on_id":"demo-multi-plexer-1py","type":"blocks","created_at":"2026-01-16T01:27:05.618112+04:00","created_by":"Edgar I."}],"comments":[{"id":6,"issue_id":"demo-multi-plexer-wp1","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-wp1'\n\n2. **Report**: bd comments add demo-multi-plexer-wp1 '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-wp1 --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments add \u003cnext-id\u003e '## Kickoff: Previous=demo-multi-plexer-wp1, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:47Z"}]}
{"id":"demo-multi-plexer-wq9","title":"Bind demo containers to 127.0.0.1 instead of 0.0.0.0","description":"## Problem\nDemo containers bind to 0.0.0.0 which exposes them on all network interfaces. If host has public IP without firewall, containers are directly accessible bypassing Caddy.\n\n## Security Impact\n- Bypasses reverse proxy security controls\n- Direct container access possible\n- Rate limiting bypassed via direct port access\n\n## File to Modify\n- `internal/container/docker.go` - Lines 77-83\n\n## Current Code\n```go\n// Lines 77-83\nportBindings := nat.PortMap{\n    containerPort: []nat.PortBinding{\n        {\n            HostIP:   \"0.0.0.0\",  // ← Binds to all interfaces\n            HostPort: strconv.Itoa(opts.Port),\n        },\n    },\n}\n```\n\n## Fixed Code\n```go\nportBindings := nat.PortMap{\n    containerPort: []nat.PortBinding{\n        {\n            HostIP:   \"127.0.0.1\",  // ← Only localhost\n            HostPort: strconv.Itoa(opts.Port),\n        },\n    },\n}\n```\n\n## Implementation Steps\n1. Change HostIP from \"0.0.0.0\" to \"127.0.0.1\" in docker.go\n2. Verify Caddy can still reach containers on localhost\n3. Apply same change to podman.go if applicable (check port binding there)\n4. Update any documentation that mentions direct container access\n\n## Verification\n1. Run `make test` - all tests pass\n2. Start server: `make dev`\n3. Provision container and note port (e.g., 32001)\n4. Verify `curl http://localhost:32001` works (local access)\n5. Verify `curl http://\u003chost-ip\u003e:32001` fails from another machine\n6. Verify Caddy proxy still works: `curl http://demo-xxx.localhost`\n\n## Dependencies\nNone","status":"open","priority":2,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:10.161627+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:09.407454+04:00","comments":[{"id":18,"issue_id":"demo-multi-plexer-wq9","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m '\u003ctype\u003e: \u003cdescription\u003e - Closes: demo-multi-plexer-wq9'\n\n2. **Report**: bd comments demo-multi-plexer-wq9 --add '## Done: \u003cfiles changed, verification done\u003e'\n\n3. **Close**: bd close demo-multi-plexer-wq9 --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add context to next issue: bd comments \u003cnext-id\u003e --add '## Kickoff: Previous=demo-multi-plexer-wq9, Start=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:55Z"}]}
{"id":"demo-multi-plexer-x9q","title":"Add Structured Logging","description":"Create pkg/logging/logger.go wrapper. Replace all log.Printf calls with leveled structured logging. Add levels: debug, info, warn, error. Use slog (Go 1.21+) or zerolog.","status":"closed","priority":2,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T16:45:19.780413+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T22:54:25.434855+04:00","closed_at":"2026-01-15T22:54:25.434855+04:00","close_reason":"Implemented structured logging using Go's slog. Created pkg/logging with a thin wrapper that adds Fatal() and Nop() helpers. Replaced all 58 log.Printf/Println/Fatalf calls with structured logging throughout the codebase. Added LOG_LEVEL and LOG_FORMAT config."}
{"id":"demo-multi-plexer-xwe","title":"Replace os.Exit in goroutine with proper shutdown signal","description":"## Problem\nIn `cmd/server/main.go:190-195`, the HTTP server goroutine calls `os.Exit(1)` on error, which bypasses ALL deferred cleanup functions.\n\n## Impact\nWhen HTTP server encounters an error:\n- Valkey connection not closed\n- NATS connections not gracefully drained\n- Replenisher not stopped\n- PrestaShop database not closed\n- Container runtime not closed\n- Orphaned containers left running\n\n## File to Modify\n- `cmd/server/main.go` - Lines 190-195\n\n## Current Code (BROKEN)\n```go\n// Line 190-195\ngo func() {\n    logger.Info(\"Server listening\", \"addr\", addr)\n    if err := srv.ListenAndServe(); err != nil \u0026\u0026 err != http.ErrServerClosed {\n        logger.Error(\"Server error\", \"error\", err)\n        os.Exit(1)  // ← BYPASSES ALL DEFER CLEANUP\n    }\n}()\n```\n\n## Fixed Code\n```go\n// Add error channel before the goroutine\nserverErrCh := make(chan error, 1)\n\ngo func() {\n    logger.Info(\"Server listening\", \"addr\", addr)\n    if err := srv.ListenAndServe(); err != nil \u0026\u0026 err != http.ErrServerClosed {\n        logger.Error(\"Server error\", \"error\", err)\n        serverErrCh \u003c- err  // Signal main goroutine instead of exit\n    }\n}()\n\n// Update select block to include server error channel\nselect {\ncase sig := \u003c-sigCh:\n    logger.Info(\"Received signal, shutting down\", \"signal\", sig)\ncase err := \u003c-serverErrCh:\n    logger.Error(\"Server failed, initiating shutdown\", \"error\", err)\n}\n```\n\n## Implementation Steps\n1. Add `serverErrCh := make(chan error, 1)` before the HTTP server goroutine\n2. Replace `os.Exit(1)` with `serverErrCh \u003c- err`\n3. Update the select block to listen on both `sigCh` and `serverErrCh`\n4. Ensure all deferred cleanup runs on either path\n\n## Verification\n1. Run `make test` - all tests pass\n2. Start server: `make dev`\n3. Simulate server error (e.g., bind to already-used port)\n4. Verify logs show proper cleanup sequence\n5. Verify no orphaned containers: `docker ps | grep demo`\n\n## Dependencies\nNone","status":"open","priority":1,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:08.955698+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:26:04.679031+04:00","comments":[{"id":5,"issue_id":"demo-multi-plexer-xwe","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m 'fix: replace os.Exit in goroutine with shutdown signal - Closes: demo-multi-plexer-xwe'\n\n2. **Report**: bd comments add demo-multi-plexer-xwe '## Done: \u003csummary of changes and files modified\u003e'\n\n3. **Close**: bd close demo-multi-plexer-xwe --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add kickoff context to next issue: bd comments add \u003cnext-id\u003e '## Kickoff: Previous=xwe, Start at=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:46Z"}]}
{"id":"demo-multi-plexer-yax","title":"Fix E2E and integration test issues","status":"closed","priority":2,"issue_type":"bug","owner":"eibrahimov@gmail.com","created_at":"2026-01-15T18:21:58.712967+04:00","created_by":"Edgar I.","updated_at":"2026-01-15T18:22:05.868426+04:00","closed_at":"2026-01-15T18:22:05.868426+04:00","close_reason":"Fixed two test issues: (1) TestLifecycle t.Cleanup() was inside acquire subtest causing premature cleanup before get/extend subtests ran, (2) Container tests used ports 32xxx which conflicted with running server's port range. Commits: 344200b, 87fbbb2"}
{"id":"demo-multi-plexer-z89","title":"Phase 4: Checkpoint Creator Tool","description":"Create cmd/checkpoint/main.go:\n- CLI tool to create PrestaShop CRIU checkpoint\n- Flags: --image, --name, --output, --mysql-host, etc.\n- Steps: start container, wait for health, warm caches, checkpoint, cleanup\n- Document usage in README or separate doc\n\nLower priority - can run manually on Linux host.","status":"closed","priority":3,"issue_type":"task","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T00:30:23.312959+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T00:44:07.083104+04:00","closed_at":"2026-01-16T00:44:07.083104+04:00","close_reason":"Checkpoint creator tool implemented with all flags. Compiles successfully."}
{"id":"demo-multi-plexer-zan","title":"Add authentication middleware to all API endpoints","description":"## Problem\nAll API endpoints are publicly accessible with zero authentication. Any user can:\n- Enumerate and access any instance details\n- Extend any instance's TTL indefinitely  \n- Delete any instance\n- View operational metrics and pool statistics\n\n## Security Impact\nCRITICAL - Production deployment blocked until resolved.\n\n## Files to Modify\n- `internal/api/handler.go` - Add auth middleware, protect routes\n- `internal/api/middleware.go` - NEW FILE: Create auth middleware\n- `internal/config/config.go` - Add API key configuration\n- `.env.example` - Add API_KEY environment variable\n\n## Current Behavior\n```go\n// internal/api/handler.go:36-78\n// All routes registered without any authentication\napi := r.Group(\"/api/v1\")\napi.POST(\"/demo/acquire\", h.acquireDemo)  // No auth\napi.GET(\"/demo/:id\", h.getDemo)           // No auth\n```\n\n## Implementation Steps\n1. Add to config.go:\n   - `APIKey string` field in config struct\n   - Load from `API_KEY` environment variable\n   - Require non-empty value in production mode\n\n2. Create `internal/api/middleware.go`:\n   ```go\n   func APIKeyAuth(apiKey string) gin.HandlerFunc {\n       return func(c *gin.Context) {\n           key := c.GetHeader(\"X-API-Key\")\n           if key == \"\" {\n               key = c.Query(\"api_key\")\n           }\n           if subtle.ConstantTimeCompare([]byte(key), []byte(apiKey)) != 1 {\n               c.AbortWithStatusJSON(401, ErrorResponse{Error: \"unauthorized\"})\n               return\n           }\n           c.Next()\n       }\n   }\n   ```\n\n3. Apply middleware to protected routes in handler.go:\n   - `/api/v1/*` routes require API key\n   - `/health` remains public\n   - `/metrics` requires API key\n\n4. Update .env.example with `API_KEY=change-me-in-production`\n\n## Verification\n1. Run `make test` - all tests pass\n2. Start server: `make dev`\n3. Test without key: `curl http://localhost:8080/api/v1/pool/stats` → 401\n4. Test with key: `curl -H \"X-API-Key: test\" http://localhost:8080/api/v1/pool/stats` → 200\n5. Health check still public: `curl http://localhost:8080/health` → 200\n\n## Dependencies\nNone - this is a foundational security feature.","status":"in_progress","priority":1,"issue_type":"feature","owner":"eibrahimov@gmail.com","created_at":"2026-01-16T01:25:07.934139+04:00","created_by":"Edgar I.","updated_at":"2026-01-16T01:53:49.23865+04:00","comments":[{"id":4,"issue_id":"demo-multi-plexer-zan","author":"Edgar I.","text":"## Completion Protocol\n\nAfter implementation verified:\n\n1. **Commit**: git add -A \u0026\u0026 git commit -m 'feat: add API authentication middleware - Closes: demo-multi-plexer-zan'\n\n2. **Report**: bd comments add demo-multi-plexer-zan '## Done: \u003csummary of changes and files modified\u003e'\n\n3. **Close**: bd close demo-multi-plexer-zan --reason='Implemented and verified'\n\n4. **Handoff**: If continuing, add kickoff context to next issue: bd comments add \u003cnext-id\u003e '## Kickoff: Previous=zan, Start at=\u003cfile:line\u003e'\n\n5. **Sync**: bd sync --flush-only","created_at":"2026-01-15T21:41:45Z"}]}
